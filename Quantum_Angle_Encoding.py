# -*- coding: utf-8 -*-
"""Copy of Merged_(1) (2).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Te7ZxCdAgM0UBylC8TXiMIgXurS9YeUs
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
from torch.autograd import Variable
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

!pip install pennylane

import pennylane as qml

!pip install tensorflow

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf

csv_path = "/content/drive/MyDrive/merged/lfccaldp_train.csv"
training_data = pd.read_csv(csv_path)

training_data

training_data.columns

training_data0 = training_data.iloc[:, 6:27]
training_data1=training_data.iloc[:, 33:53]
training_data=pd.concat([training_data0,training_data1],axis=1,ignore_index=False)

training_data.columns

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
training_data['KEY'] = label_encoder.fit_transform(training_data['KEY'])
training_data

csv_path = "/content/drive/MyDrive/merged/lfccaldp_dev.csv"
val_data = pd.read_csv(csv_path)
val_data0 = val_data.iloc[:, 6:27]
val_data1=val_data.iloc[:, 33:53]
val_data=pd.concat([val_data0,val_data1],axis=1,ignore_index=False)
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
val_data['KEY'] = label_encoder.fit_transform(val_data['KEY'])
val_data

training_data=pd.concat([training_data,val_data],axis=0,ignore_index=True)

training_data

csv_path = "/content/drive/MyDrive/merged/lfccaldp_eval.csv"
test_data = pd.read_csv(csv_path)
test_data0 = test_data.iloc[:, 6:27]
test_data1=test_data.iloc[:, 33:53]
test_data=pd.concat([test_data0,test_data1],axis=1,ignore_index=False)
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
test_data['KEY'] = label_encoder.fit_transform(test_data['KEY'])
test_data

X_train = training_data.iloc[:, 1:41]
X_train.shape

y_train = training_data.iloc[:, 0]
y_train.shape

X_val = val_data.iloc[:, 1:41]
y_val = val_data.iloc[:, 0]

X_test = val_data.iloc[:, 1:41]
y_test = val_data.iloc[:, 0]

X_train = torch.tensor(X_train.values)
y_train = torch.tensor(y_train.values)
X_val = torch.tensor(X_val.values)
y_val = torch.tensor(y_val.values)
X_test = torch.tensor(X_test.values)
y_test = torch.tensor(y_test.values)

X_train.dtype

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
device

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train= torch.tensor(y_train, dtype=torch.float32)
X_test= torch.tensor(X_test, dtype=torch.float32)
y_test= torch.tensor(y_test, dtype=torch.float32)

n_qubits =4          # Number of qubits
num_epochs =15          # Number of training epochs
q_depth =1               # Depth of the quantum circuit (number of variational layers)
gamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.
q_delta = 0.1              # Initial spread of random quantum weights

dev = qml.device("default.qubit", wires=n_qubits)

def H_layer(nqubits):
    """Layer of single-qubit Hadamard gates.
    """
    for idx in range(nqubits):
        qml.Hadamard(wires=idx)


def RY_layer(w):
    """Layer of parametrized qubit rotations around the y axis.
    """
    for idx, element in enumerate(w):
        qml.RY(element, wires=idx)


def entangling_layer(nqubits):
  for i in range(0, nqubits - 1, 2):
    qml.CNOT(wires=[i, i + 1])
  for i in range(1, nqubits - 1, 2):
    qml.CNOT(wires=[i, i + 1])

@qml.qnode(dev,interface="torch")
def quantum_net(q_input_features, q_weights_flat):


    # Reshape weights
    q_weights = q_weights_flat.reshape(q_depth, n_qubits)

    # Start from state |+> , unbiased w.r.t. |0> and |1>
    H_layer(n_qubits)

    # Embed features in the quantum node
    RY_layer(q_input_features)

    # Sequence of trainable variational layers
    for k in range(q_depth):
        entangling_layer(n_qubits)
        RY_layer(q_weights[k])

    # Expectation values in the Z basis
    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]
    return tuple(exp_vals)

class DressedQuantumNet(nn.Module):
    """
    Torch module implementing the *dressed* quantum net.
    """

    def __init__(self):
        """
        Definition of the *dressed* layout.
        """

        super().__init__()
        self.pre_net = nn.Linear(512, n_qubits)
        self.relu=nn.ReLU()
        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))
        self.post_net = nn.Linear(n_qubits,1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, input_features):
        """
        Defining how tensors are supposed to move through the *dressed* quantum
        net.
        """

        # obtain the input features for the quantum circuit
        # by reducing the feature dimension from 512 to 4
        pre_out = self.pre_net(input_features)
        #q_in = torch.tanh(pre_out) * np.pi / 2.0
        q_in=self.relu(pre_out)

        # Apply the quantum circuit to each element of the batch and append to q_out
        q_out = torch.Tensor(0, n_qubits)
        q_out = q_out.to(device)
        for elem in q_in:
            q_out_elem = torch.hstack(quantum_net(elem, self.q_params)).float().unsqueeze(0)
            q_out = torch.cat((q_out, q_out_elem))

        # return the two-dimensional prediction from the postprocessing layer
        out=self.post_net(q_out)
        out = out.squeeze(1)
        return out

class RNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, num_classes):
        super(RNN, self).__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.bilstm1 = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True).to(device)
        self.bilstm2 = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True).to(device)
        self.dropout1 = nn.Dropout(0.5)
        self.dropout2 = nn.Dropout(0.5)
        self.Batch1= nn.BatchNorm1d(hidden_size * 2)
        self.Batch2= nn.BatchNorm1d(hidden_size * 2)
        self.fc1 = DressedQuantumNet()
        """self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, 128)
        self.fc4 = nn.Linear(128, 64)
        self.fc5 = nn.Linear(64, 32)
        self.fc6 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()"""

    def forward(self, x):
        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)
        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)
        out, _ = self.bilstm1(x, (h0, c0))

        out = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)
        out=self.Batch1(out)
        out = self.dropout1(out)
        h1 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)
        c1 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)
        out, _ = self.bilstm2(x, (h1, c1))
        out = torch.cat((out[:, -1, :self.hidden_size], out[:, 0, self.hidden_size:]), dim=1)
        out=self.Batch2(out)
        out = self.dropout2(out)
        out = torch.relu(self.fc1(out)).to(device)
        return out

        """out = torch.relu(self.fc2(out)).to(device)
        out = torch.relu(self.fc3(out)).to(device)
        out = torch.relu(self.fc4(out)).to(device)
        out = torch.relu(self.fc5(out)).to(device)
        out = torch.relu(self.fc6(out)).to(device)
        out=  self.sigmoid(out)
        out = out.squeeze(1)"""
        #return out

num_epochs = 150
learning_rate = 0.00001
input_size = 40
hidden_size = 256
num_layers = 3
num_classes = 2
lstm = RNN(input_size, hidden_size, num_layers, num_classes)
lstm.to(device)

print(lstm)

criterion=nn.BCELoss()
optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate,weight_decay=0.001)

import torch
from torch.utils.data import DataLoader, TensorDataset
train_dataset = TensorDataset(X_train, y_train)
train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)
test_dataset = TensorDataset(X_test, y_test)
test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=True)

num_epochs = 150
threshold = 0.5
tacc=[]
tloss=[]
vacc=[]
vloss=[]


for epoch in range(num_epochs):
    lstm.train()
    total_loss = 0
    total_correct = 0
    total_samples = 0

    for batch_x_indices, batch_y in train_loader:
        batch_x_indices = batch_x_indices.to(device)
        batch_y = batch_y.to(device)
        optimizer.zero_grad()
        batch_x_indices = batch_x_indices.unsqueeze(1)
        outputs = lstm(batch_x_indices)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        predicted = (outputs.data > threshold).type(torch.long)
        total_samples += batch_y.size(0)
        total_correct += (predicted == batch_y).sum().item()

    avg_train_loss = total_loss / len(train_loader)
    tloss.append(avg_train_loss)
    train_accuracy = total_correct / total_samples
    tacc.append(train_accuracy)
    if epoch%5==0:
        print(f"Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.2%}")

    # Validation
    lstm.eval()
    correct = 0
    total = 0
    val_loss = 0

    with torch.no_grad():
      val_loss = []
      for batch_x_indices, batch_y in test_loader:

        batch_x_indices = batch_x_indices.to(device)
        batch_y = batch_y.to(device)
        batch_x_indices = batch_x_indices.unsqueeze(1)
        outputs = lstm(batch_x_indices)
        loss = criterion(outputs, batch_y)

        val_loss.append(loss.item())  # Append the loss to the list

        predicted = (outputs.data > threshold).type(torch.long)
        total += batch_y.size(0)
        correct += (predicted == batch_y).sum().item()
    avg_val_loss = sum(val_loss) / len(val_loss)  # Calculate average loss
    vloss.append(avg_val_loss)
    val_accuracy = correct / total
    vacc.append(val_accuracy )
    if epoch%5==0:
        print(f"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.2%}")

import matplotlib.pyplot as plt
epochs=range(1,(183+1))
plt.plot(epochs,tacc,'r',label="Train Acc")
plt.plot(epochs,vacc,'b',label="Val Acc")
plt.title("Training vs Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend(["Train Acc","Val Acc"])
plt.show()

import matplotlib.pyplot as plt
epochs=range(1,(183+1))
plt.plot(epochs,tloss,'r',label="Train Loss")
plt.plot(epochs,vloss,'b',label="Val Loss")
plt.title("Training vs Validation Loss")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend(["Train Loss","Val Loss"])
plt.show()

import numpy as np
from sklearn.metrics import roc_curve
from scipy.optimize import brentq
from scipy.interpolate import interp1d
X_test=X_test.to(device)
X_test= X_test.unsqueeze(1)
outputs= lstm(X_test)
y_pred = (outputs.data > 0.5).type(torch.long)
y_pred_cpu = y_pred.cpu()
y_pred= y_pred_cpu.numpy()
fpr, tpr, thresholds = roc_curve(y_test, y_pred)
eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)
print(f"Equal Error Rate (EER): {eer:.4f}")